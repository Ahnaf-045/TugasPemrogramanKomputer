{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXTAEFCWdu1rgcm0xZBtD4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahnaf-045/TugasPemrogramanKomputer/blob/main/Tugas_Minggu_12_Kelompok_10_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwo5aDZGMvU4",
        "outputId": "b9860946-99c5-42c1-8b25-8dc9e92223ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas) (0.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from geopandas) (24.2)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (3.7.1)\n",
            "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.1.0)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.4.26)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.2.0)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ],
      "source": [
        "pip install pandas geopandas rasterio numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from rasterio.sample import sample_gen\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def download_and_process_data():\n",
        "    \"\"\"\n",
        "    Process geospatial data from GeoTIFF, GPKG, and CSV files.\n",
        "\n",
        "    Note: Download the files from Google Drive first:\n",
        "    1. GeoTIFF: https://drive.google.com/file/d/1-Zdq_ZXj4WoX5ubQnoHRjSlHfHrx07x0/view?usp=sharing\n",
        "    2. GPKG: https://drive.google.com/file/d/1AQcHlqzmpQIukLyPIkUnb04-pbjoET_y/view?usp=sharing\n",
        "    3. CSV: https://drive.google.com/file/d/1v8_2PwMxl1QGdcbC_RPgUGtYSZsmJDri/view?usp=sharing\n",
        "    \"\"\"\n",
        "\n",
        "    # File paths (update these to match your downloaded files)\n",
        "    geotiff_path = \"your_geotiff_file.tif\"  # Replace with actual filename\n",
        "    gpkg_path = \"your_survey_data.gpkg\"     # Replace with actual filename\n",
        "    csv_path = \"perioda.csv\"                # Replace with actual filename\n",
        "\n",
        "    print(\"Starting geospatial data processing...\")\n",
        "\n",
        "    # Step 1: Load survey data from GPKG\n",
        "    print(\"Loading survey data from GPKG...\")\n",
        "    try:\n",
        "        survey_data = gpd.read_file(gpkg_path)\n",
        "        print(f\"Survey data loaded: {len(survey_data)} records\")\n",
        "        print(\"Columns:\", survey_data.columns.tolist())\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading GPKG: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Step 2: Load period lookup table\n",
        "    print(\"Loading period lookup table...\")\n",
        "    try:\n",
        "        period_lookup = pd.read_csv(csv_path)\n",
        "        print(f\"Period lookup loaded: {len(period_lookup)} records\")\n",
        "        print(\"Columns:\", period_lookup.columns.tolist())\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Step 3: Filter and prepare survey data\n",
        "    print(\"Filtering survey data...\")\n",
        "\n",
        "    # Extract coordinates (assuming geometry column exists)\n",
        "    if 'geometry' in survey_data.columns:\n",
        "        survey_data['lon'] = survey_data.geometry.x\n",
        "        survey_data['lat'] = survey_data.geometry.y\n",
        "\n",
        "    # Create filtered table with required columns\n",
        "    # Adjust column names based on your actual data structure\n",
        "    required_cols = ['fid', 'lon', 'lat', 'date', 'fase']\n",
        "\n",
        "    # Check which columns exist and map them\n",
        "    available_cols = survey_data.columns.tolist()\n",
        "    print(f\"Available columns: {available_cols}\")\n",
        "\n",
        "    # Create mapping for common column name variations\n",
        "    column_mapping = {}\n",
        "    for col in required_cols:\n",
        "        if col in available_cols:\n",
        "            column_mapping[col] = col\n",
        "        elif col.upper() in available_cols:\n",
        "            column_mapping[col] = col.upper()\n",
        "        elif col.lower() in available_cols:\n",
        "            column_mapping[col] = col.lower()\n",
        "        elif col == 'fid' and 'FID' in available_cols:\n",
        "            column_mapping[col] = 'FID'\n",
        "        elif col == 'fid' and 'id' in available_cols:\n",
        "            column_mapping[col] = 'id'\n",
        "        elif col == 'date' and 'Date' in available_cols:\n",
        "            column_mapping[col] = 'Date'\n",
        "        elif col == 'fase' and 'phase' in available_cols:\n",
        "            column_mapping[col] = 'phase'\n",
        "\n",
        "    # Filter data with existing columns\n",
        "    filtered_data = survey_data.copy()\n",
        "\n",
        "    # Rename columns to standard names\n",
        "    for new_name, old_name in column_mapping.items():\n",
        "        if old_name in filtered_data.columns:\n",
        "            filtered_data = filtered_data.rename(columns={old_name: new_name})\n",
        "\n",
        "    # Select only required columns that exist\n",
        "    existing_cols = [col for col in required_cols if col in filtered_data.columns]\n",
        "    filtered_survey = filtered_data[existing_cols + ['geometry'] if 'geometry' in filtered_data.columns else existing_cols].copy()\n",
        "\n",
        "    print(f\"Filtered survey data: {len(filtered_survey)} records\")\n",
        "    print(\"Table 1 - Filtered Survey Data:\")\n",
        "    print(filtered_survey.head())\n",
        "\n",
        "    # Step 4: Add period information\n",
        "    print(\"Adding period information...\")\n",
        "\n",
        "    # Merge with period lookup table\n",
        "    # Adjust merge column based on your data structure\n",
        "    if 'date' in filtered_survey.columns and len(period_lookup.columns) >= 2:\n",
        "        # Assuming period lookup has date and period columns\n",
        "        period_col_names = period_lookup.columns.tolist()\n",
        "\n",
        "        # Convert date columns to datetime for proper matching\n",
        "        if 'date' in filtered_survey.columns:\n",
        "            try:\n",
        "                filtered_survey['date'] = pd.to_datetime(filtered_survey['date'])\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Merge based on the first column of period lookup (assuming it's the key)\n",
        "        merge_key = period_col_names[0]\n",
        "        period_key = period_col_names[1] if len(period_col_names) > 1 else 'periode'\n",
        "\n",
        "        try:\n",
        "            # Try different merge strategies\n",
        "            if merge_key in filtered_survey.columns:\n",
        "                survey_with_period = filtered_survey.merge(\n",
        "                    period_lookup,\n",
        "                    left_on=merge_key,\n",
        "                    right_on=period_col_names[0],\n",
        "                    how='left'\n",
        "                )\n",
        "            else:\n",
        "                # Create a simple period assignment based on date ranges or other logic\n",
        "                survey_with_period = filtered_survey.copy()\n",
        "                survey_with_period['periode'] = 1  # Default period\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Merge error: {e}\")\n",
        "            survey_with_period = filtered_survey.copy()\n",
        "            survey_with_period['periode'] = 1  # Default period\n",
        "    else:\n",
        "        survey_with_period = filtered_survey.copy()\n",
        "        survey_with_period['periode'] = 1  # Default period\n",
        "\n",
        "    print(\"Table 2 - Survey Data with Period:\")\n",
        "    print(survey_with_period.head())\n",
        "\n",
        "    # Step 5: Extract raster values\n",
        "    print(\"Extracting raster values...\")\n",
        "\n",
        "    try:\n",
        "        with rasterio.open(geotiff_path) as src:\n",
        "            print(f\"Raster info: {src.count} bands, {src.width}x{src.height} pixels\")\n",
        "\n",
        "            # Prepare coordinates for sampling\n",
        "            coords = [(lon, lat) for lon, lat in zip(survey_with_period['lon'], survey_with_period['lat'])]\n",
        "\n",
        "            # Extract values for each point and each period\n",
        "            final_results = []\n",
        "\n",
        "            for idx, row in survey_with_period.iterrows():\n",
        "                try:\n",
        "                    coord = (row['lon'], row['lat'])\n",
        "                    periode = int(row.get('periode', 1))\n",
        "\n",
        "                    # Ensure period is within valid band range\n",
        "                    if periode > src.count:\n",
        "                        periode = 1\n",
        "\n",
        "                    # Sample the raster at the point location for the specific band\n",
        "                    sampled_values = list(sample_gen(src, [coord], indexes=[periode]))\n",
        "                    p0_value = sampled_values[0][0] if sampled_values and len(sampled_values[0]) > 0 else np.nan\n",
        "\n",
        "                    # Create result row\n",
        "                    result_row = {\n",
        "                        'fid': row.get('fid', idx),\n",
        "                        'lon': row['lon'],\n",
        "                        'lat': row['lat'],\n",
        "                        'date': row.get('date', ''),\n",
        "                        'fase': row.get('fase', ''),\n",
        "                        'periode': periode,\n",
        "                        'p0': p0_value\n",
        "                    }\n",
        "\n",
        "                    final_results.append(result_row)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing point {idx}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Create final DataFrame\n",
        "            final_df = pd.DataFrame(final_results)\n",
        "\n",
        "            print(\"Table 3 - Final Results with Raster Values:\")\n",
        "            print(final_df.head())\n",
        "            print(f\"\\nFinal dataset: {len(final_df)} records\")\n",
        "\n",
        "            # Save results\n",
        "            output_file = \"processed_survey_data.csv\"\n",
        "            final_df.to_csv(output_file, index=False)\n",
        "            print(f\"Results saved to: {output_file}\")\n",
        "\n",
        "            return final_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing raster: {e}\")\n",
        "        return survey_with_period\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the data processing pipeline.\n",
        "    \"\"\"\n",
        "    print(\"=== Geospatial Data Processing Pipeline ===\")\n",
        "    print()\n",
        "    print(\"Before running this script, please:\")\n",
        "    print(\"1. Download the GeoTIFF file from Google Drive\")\n",
        "    print(\"2. Download the GPKG survey data file from Google Drive\")\n",
        "    print(\"3. Download the perioda.csv file from Google Drive\")\n",
        "    print(\"4. Update the file paths in the script\")\n",
        "    print()\n",
        "\n",
        "    # Check if files exist\n",
        "    files_to_check = [\"your_geotiff_file.tif\", \"your_survey_data.gpkg\", \"perioda.csv\"]\n",
        "    missing_files = [f for f in files_to_check if not os.path.exists(f)]\n",
        "\n",
        "    if missing_files:\n",
        "        print(\"Missing files:\")\n",
        "        for f in missing_files:\n",
        "            print(f\"  - {f}\")\n",
        "        print(\"\\nPlease download and update file paths before running.\")\n",
        "        return\n",
        "\n",
        "    # Process the data\n",
        "    result = download_and_process_data()\n",
        "\n",
        "    if result is not None:\n",
        "        print(\"\\n=== Processing Complete ===\")\n",
        "        print(\"Three tables have been generated:\")\n",
        "        print(\"1. Filtered survey data (fid, lon, lat, date, fase)\")\n",
        "        print(\"2. Survey data with period (fid, lon, lat, date, fase, periode)\")\n",
        "        print(\"3. Final data with raster values (fid, lon, lat, date, fase, periode, p0)\")\n",
        "    else:\n",
        "        print(\"Processing failed. Please check file paths and data format.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "# Alternative function for manual file processing\n",
        "def process_with_custom_paths(geotiff_path, gpkg_path, csv_path):\n",
        "    \"\"\"\n",
        "    Process data with custom file paths.\n",
        "\n",
        "    Args:\n",
        "        geotiff_path (str): Path to GeoTIFF file\n",
        "        gpkg_path (str): Path to GPKG file\n",
        "        csv_path (str): Path to CSV file\n",
        "    \"\"\"\n",
        "    # Update the file paths in the global scope\n",
        "    globals()['geotiff_path'] = geotiff_path\n",
        "    globals()['gpkg_path'] = gpkg_path\n",
        "    globals()['csv_path'] = csv_path\n",
        "\n",
        "    return download_and_process_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NHprCAOP2wQ",
        "outputId": "9622c51c-197d-47f9-bdc1-713d3f728348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Geospatial Data Processing Pipeline ===\n",
            "\n",
            "Before running this script, please:\n",
            "1. Download the GeoTIFF file from Google Drive\n",
            "2. Download the GPKG survey data file from Google Drive\n",
            "3. Download the perioda.csv file from Google Drive\n",
            "4. Update the file paths in the script\n",
            "\n",
            "Missing files:\n",
            "  - your_geotiff_file.tif\n",
            "  - your_survey_data.gpkg\n",
            "  - perioda.csv\n",
            "\n",
            "Please download and update file paths before running.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from rasterio.sample import sample_gen\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "\n",
        "def download_from_gdrive(file_id, output_path):\n",
        "    \"\"\"\n",
        "    Download file from Google Drive using file ID.\n",
        "\n",
        "    Args:\n",
        "        file_id (str): Google Drive file ID\n",
        "        output_path (str): Local path to save the file\n",
        "    \"\"\"\n",
        "    print(f\"Downloading file ID: {file_id}\")\n",
        "\n",
        "    # Google Drive download URL\n",
        "    url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "    session = requests.Session()\n",
        "    response = session.get(url, stream=True)\n",
        "\n",
        "    # Handle Google Drive's virus scan warning for large files\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            params = {'id': file_id, 'confirm': value}\n",
        "            response = session.get(url, params=params, stream=True)\n",
        "            break\n",
        "\n",
        "    # Save the file\n",
        "    with open(output_path, 'wb') as f:\n",
        "        for chunk in response.iter_content(32768):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "    print(f\"Downloaded: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "def extract_file_id_from_url(drive_url):\n",
        "    \"\"\"\n",
        "    Extract file ID from Google Drive sharing URL.\n",
        "\n",
        "    Args:\n",
        "        drive_url (str): Google Drive sharing URL\n",
        "\n",
        "    Returns:\n",
        "        str: File ID\n",
        "    \"\"\"\n",
        "    if '/file/d/' in drive_url:\n",
        "        return drive_url.split('/file/d/')[1].split('/')[0]\n",
        "    elif 'id=' in drive_url:\n",
        "        return drive_url.split('id=')[1].split('&')[0]\n",
        "    else:\n",
        "        raise ValueError(\"Cannot extract file ID from URL\")\n",
        "\n",
        "def download_and_process_data():\n",
        "    \"\"\"\n",
        "    Download and process geospatial data from Google Drive links.\n",
        "    \"\"\"\n",
        "\n",
        "    # Google Drive URLs (from your request)\n",
        "    geotiff_url = \"https://drive.google.com/file/d/1-Zdq_ZXj4WoX5ubQnoHRjSlHfHrx07x0/view?usp=sharing\"\n",
        "    gpkg_url = \"https://drive.google.com/file/d/1AQcHlqzmpQIukLyPIkUnb04-pbjoET_y/view?usp=sharing\"\n",
        "    csv_url = \"https://drive.google.com/file/d/1v8_2PwMxl1QGdcbC_RPgUGtYSZsmJDri/view?usp=sharing\"\n",
        "\n",
        "    # Extract file IDs\n",
        "    geotiff_id = extract_file_id_from_url(geotiff_url)\n",
        "    gpkg_id = extract_file_id_from_url(gpkg_url)\n",
        "    csv_id = extract_file_id_from_url(csv_url)\n",
        "\n",
        "    # Local file paths\n",
        "    geotiff_path = \"downloaded_geotiff.tif\"\n",
        "    gpkg_path = \"downloaded_survey.gpkg\"\n",
        "    csv_path = \"perioda.csv\"\n",
        "\n",
        "    print(\"Starting download and processing...\")\n",
        "\n",
        "    # Download files\n",
        "    try:\n",
        "        print(\"Downloading GeoTIFF...\")\n",
        "        download_from_gdrive(geotiff_id, geotiff_path)\n",
        "\n",
        "        print(\"Downloading GPKG...\")\n",
        "        download_from_gdrive(gpkg_id, gpkg_path)\n",
        "\n",
        "        print(\"Downloading CSV...\")\n",
        "        download_from_gdrive(csv_id, csv_path)\n",
        "\n",
        "        print(\"All files downloaded successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Download error: {e}\")\n",
        "        print(\"Trying alternative download method...\")\n",
        "\n",
        "        # Alternative method using requests with different headers\n",
        "        try:\n",
        "            download_with_session(geotiff_url, geotiff_path)\n",
        "            download_with_session(gpkg_url, gpkg_path)\n",
        "            download_with_session(csv_url, csv_path)\n",
        "        except Exception as e2:\n",
        "            print(f\"Alternative download also failed: {e2}\")\n",
        "            return None\n",
        "\n",
        "    # Step 1: Load survey data from GPKG\n",
        "    print(\"\\nLoading survey data from GPKG...\")\n",
        "    try:\n",
        "        survey_data = gpd.read_file(gpkg_path)\n",
        "        print(f\"Survey data loaded: {len(survey_data)} records\")\n",
        "        print(\"Columns:\", survey_data.columns.tolist())\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading GPKG: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Step 2: Load period lookup table\n",
        "    print(\"Loading period lookup table...\")\n",
        "    try:\n",
        "        period_lookup = pd.read_csv(csv_path)\n",
        "        print(f\"Period lookup loaded: {len(period_lookup)} records\")\n",
        "        print(\"Columns:\", period_lookup.columns.tolist())\n",
        "        print(\"Sample data:\")\n",
        "        print(period_lookup.head())\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Step 3: Filter and prepare survey data\n",
        "    print(\"\\nFiltering survey data...\")\n",
        "\n",
        "    # Extract coordinates (assuming geometry column exists)\n",
        "    if 'geometry' in survey_data.columns:\n",
        "        survey_data['lon'] = survey_data.geometry.x\n",
        "        survey_data['lat'] = survey_data.geometry.y\n",
        "\n",
        "    # Create filtered table with required columns\n",
        "    required_cols = ['fid', 'lon', 'lat', 'date', 'fase']\n",
        "\n",
        "    # Check which columns exist and map them\n",
        "    available_cols = survey_data.columns.tolist()\n",
        "    print(f\"Available columns: {available_cols}\")\n",
        "\n",
        "    # Create mapping for common column name variations\n",
        "    column_mapping = {}\n",
        "    for col in required_cols:\n",
        "        if col in available_cols:\n",
        "            column_mapping[col] = col\n",
        "        elif col.upper() in available_cols:\n",
        "            column_mapping[col] = col.upper()\n",
        "        elif col.lower() in available_cols:\n",
        "            column_mapping[col] = col.lower()\n",
        "        elif col == 'fid':\n",
        "            # Try common ID column names\n",
        "            for id_col in ['FID', 'id', 'ID', 'index', 'INDEX']:\n",
        "                if id_col in available_cols:\n",
        "                    column_mapping[col] = id_col\n",
        "                    break\n",
        "        elif col == 'date':\n",
        "            # Try common date column names\n",
        "            for date_col in ['Date', 'DATE', 'tanggal', 'waktu', 'time']:\n",
        "                if date_col in available_cols:\n",
        "                    column_mapping[col] = date_col\n",
        "                    break\n",
        "        elif col == 'fase':\n",
        "            # Try common phase column names\n",
        "            for phase_col in ['phase', 'Phase', 'FASE', 'PHASE']:\n",
        "                if phase_col in available_cols:\n",
        "                    column_mapping[col] = phase_col\n",
        "                    break\n",
        "\n",
        "    # Add missing columns with default values if not found\n",
        "    filtered_data = survey_data.copy()\n",
        "\n",
        "    # Add FID if not exists\n",
        "    if 'fid' not in column_mapping and len(filtered_data) > 0:\n",
        "        filtered_data['fid'] = range(1, len(filtered_data) + 1)\n",
        "        column_mapping['fid'] = 'fid'\n",
        "\n",
        "    # Rename columns to standard names\n",
        "    for new_name, old_name in column_mapping.items():\n",
        "        if old_name in filtered_data.columns and old_name != new_name:\n",
        "            filtered_data = filtered_data.rename(columns={old_name: new_name})\n",
        "\n",
        "    # Select required columns\n",
        "    final_cols = []\n",
        "    for col in required_cols:\n",
        "        if col in filtered_data.columns:\n",
        "            final_cols.append(col)\n",
        "        else:\n",
        "            # Add default column if missing\n",
        "            if col == 'fase':\n",
        "                filtered_data[col] = 'unknown'\n",
        "            elif col == 'date':\n",
        "                filtered_data[col] = datetime.now().strftime('%Y-%m-%d')\n",
        "            final_cols.append(col)\n",
        "\n",
        "    filtered_survey = filtered_data[final_cols].copy()\n",
        "\n",
        "    print(f\"\\nTABEL 1 - Filtered survey data: {len(filtered_survey)} records\")\n",
        "    print(\"Columns:\", filtered_survey.columns.tolist())\n",
        "    print(filtered_survey.head())\n",
        "\n",
        "    # Step 4: Add period information\n",
        "    print(\"\\nAdding period information...\")\n",
        "\n",
        "    # Check period lookup structure\n",
        "    period_cols = period_lookup.columns.tolist()\n",
        "    print(f\"Period lookup columns: {period_cols}\")\n",
        "\n",
        "    # Try to merge with period lookup\n",
        "    survey_with_period = filtered_survey.copy()\n",
        "\n",
        "    # Simple period assignment based on available data\n",
        "    if len(period_cols) >= 2:\n",
        "        # Assume first column is key, second is period\n",
        "        try:\n",
        "            # Try different merge strategies\n",
        "            merge_successful = False\n",
        "\n",
        "            # Strategy 1: Direct merge on date\n",
        "            if 'date' in filtered_survey.columns and period_cols[0].lower() in ['date', 'tanggal', 'waktu']:\n",
        "                try:\n",
        "                    survey_with_period = filtered_survey.merge(\n",
        "                        period_lookup,\n",
        "                        left_on='date',\n",
        "                        right_on=period_cols[0],\n",
        "                        how='left'\n",
        "                    )\n",
        "                    survey_with_period = survey_with_period.rename(columns={period_cols[1]: 'periode'})\n",
        "                    merge_successful = True\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Strategy 2: Assign period based on row index or other logic\n",
        "            if not merge_successful:\n",
        "                # Create period assignment based on data distribution\n",
        "                n_records = len(survey_with_period)\n",
        "                n_periods = len(period_lookup)\n",
        "\n",
        "                if n_periods > 0:\n",
        "                    # Distribute records across periods\n",
        "                    records_per_period = max(1, n_records // n_periods)\n",
        "                    periods = []\n",
        "                    for i in range(n_records):\n",
        "                        period_idx = min(i // records_per_period, n_periods - 1)\n",
        "                        periods.append(period_idx + 1)  # 1-based indexing\n",
        "                    survey_with_period['periode'] = periods\n",
        "                else:\n",
        "                    survey_with_period['periode'] = 1\n",
        "        except Exception as e:\n",
        "            print(f\"Period merge error: {e}\")\n",
        "            survey_with_period['periode'] = 1\n",
        "    else:\n",
        "        survey_with_period['periode'] = 1\n",
        "\n",
        "    print(f\"\\nTABEL 2 - Survey data with period: {len(survey_with_period)} records\")\n",
        "    print(\"Columns:\", survey_with_period.columns.tolist())\n",
        "    print(survey_with_period.head())\n",
        "\n",
        "    # Step 5: Extract raster values\n",
        "    print(\"\\nExtracting raster values...\")\n",
        "\n",
        "    try:\n",
        "        with rasterio.open(geotiff_path) as src:\n",
        "            print(f\"Raster info: {src.count} bands, {src.width}x{src.height} pixels\")\n",
        "            print(f\"CRS: {src.crs}\")\n",
        "            print(f\"Bounds: {src.bounds}\")\n",
        "\n",
        "            # Extract values for each point\n",
        "            final_results = []\n",
        "\n",
        "            for idx, row in survey_with_period.iterrows():\n",
        "                try:\n",
        "                    lon = float(row['lon'])\n",
        "                    lat = float(row['lat'])\n",
        "                    coord = (lon, lat)\n",
        "                    periode = int(row.get('periode', 1))\n",
        "\n",
        "                    # Ensure period is within valid band range\n",
        "                    if periode < 1:\n",
        "                        periode = 1\n",
        "                    elif periode > src.count:\n",
        "                        periode = src.count\n",
        "\n",
        "                    # Sample the raster at the point location\n",
        "                    sampled_values = list(sample_gen(src, [coord], indexes=[periode]))\n",
        "\n",
        "                    if sampled_values and len(sampled_values) > 0 and len(sampled_values[0]) > 0:\n",
        "                        p0_value = float(sampled_values[0][0])\n",
        "                        # Handle NoData values\n",
        "                        if np.isnan(p0_value) or p0_value == src.nodata:\n",
        "                            p0_value = None\n",
        "                    else:\n",
        "                        p0_value = None\n",
        "\n",
        "                    # Create result row\n",
        "                    result_row = {\n",
        "                        'fid': row['fid'],\n",
        "                        'lon': lon,\n",
        "                        'lat': lat,\n",
        "                        'date': row['date'],\n",
        "                        'fase': row['fase'],\n",
        "                        'periode': periode,\n",
        "                        'p0': p0_value\n",
        "                    }\n",
        "\n",
        "                    final_results.append(result_row)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing point {idx}: {e}\")\n",
        "                    # Still add the row with null p0\n",
        "                    result_row = {\n",
        "                        'fid': row.get('fid', idx),\n",
        "                        'lon': row.get('lon', 0),\n",
        "                        'lat': row.get('lat', 0),\n",
        "                        'date': row.get('date', ''),\n",
        "                        'fase': row.get('fase', ''),\n",
        "                        'periode': row.get('periode', 1),\n",
        "                        'p0': None\n",
        "                    }\n",
        "                    final_results.append(result_row)\n",
        "\n",
        "            # Create final DataFrame\n",
        "            final_df = pd.DataFrame(final_results)\n",
        "\n",
        "            print(f\"\\nTABEL 3 - Final results with raster values: {len(final_df)} records\")\n",
        "            print(\"Columns:\", final_df.columns.tolist())\n",
        "            print(final_df.head())\n",
        "\n",
        "            # Statistics\n",
        "            valid_p0 = final_df['p0'].notna().sum()\n",
        "            print(f\"\\nStatistics:\")\n",
        "            print(f\"Total records: {len(final_df)}\")\n",
        "            print(f\"Valid p0 values: {valid_p0}\")\n",
        "            print(f\"Null p0 values: {len(final_df) - valid_p0}\")\n",
        "\n",
        "            if valid_p0 > 0:\n",
        "                print(f\"P0 range: {final_df['p0'].min():.2f} to {final_df['p0'].max():.2f}\")\n",
        "\n",
        "            # Save results\n",
        "            output_file = \"processed_survey_data.csv\"\n",
        "            final_df.to_csv(output_file, index=False)\n",
        "            print(f\"\\nResults saved to: {output_file}\")\n",
        "\n",
        "            # Clean up downloaded files\n",
        "            cleanup_files = [geotiff_path, gpkg_path, csv_path]\n",
        "            for file in cleanup_files:\n",
        "                try:\n",
        "                    if os.path.exists(file):\n",
        "                        os.remove(file)\n",
        "                        print(f\"Cleaned up: {file}\")\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            return final_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing raster: {e}\")\n",
        "        print(\"Returning data without raster values...\")\n",
        "        survey_with_period['p0'] = None\n",
        "        return survey_with_period\n",
        "\n",
        "def download_with_session(url, output_path):\n",
        "    \"\"\"\n",
        "    Alternative download method using requests session.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "    }\n",
        "\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers)\n",
        "\n",
        "    response = session.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    with open(output_path, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the data processing pipeline.\n",
        "    \"\"\"\n",
        "    print(\"=== Geospatial Data Processing Pipeline ===\")\n",
        "    print(\"Downloading data from Google Drive and processing...\")\n",
        "    print()\n",
        "\n",
        "    # Process the data\n",
        "    result = download_and_process_data()\n",
        "\n",
        "    if result is not None:\n",
        "        print(\"\\n=== Processing Complete ===\")\n",
        "        print(\"Tiga tabel telah berhasil dibuat:\")\n",
        "        print(\"1. Tabel 1: Data survey terfilter (fid, lon, lat, date, fase)\")\n",
        "        print(\"2. Tabel 2: Data survey dengan periode (fid, lon, lat, date, fase, periode)\")\n",
        "        print(\"3. Tabel 3: Data final dengan nilai raster (fid, lon, lat, date, fase, periode, p0)\")\n",
        "        print(\"\\nFile hasil disimpan sebagai: processed_survey_data.csv\")\n",
        "    else:\n",
        "        print(\"Processing gagal. Silakan periksa koneksi internet dan link Google Drive.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqMsSSu3QqME",
        "outputId": "9e229cfa-7c0b-4752-9f67-91b80eb16ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Geospatial Data Processing Pipeline ===\n",
            "Downloading data from Google Drive and processing...\n",
            "\n",
            "Starting download and processing...\n",
            "Downloading GeoTIFF...\n",
            "Downloading file ID: 1-Zdq_ZXj4WoX5ubQnoHRjSlHfHrx07x0\n",
            "Downloaded: downloaded_geotiff.tif\n",
            "Downloading GPKG...\n",
            "Downloading file ID: 1AQcHlqzmpQIukLyPIkUnb04-pbjoET_y\n",
            "Downloaded: downloaded_survey.gpkg\n",
            "Downloading CSV...\n",
            "Downloading file ID: 1v8_2PwMxl1QGdcbC_RPgUGtYSZsmJDri\n",
            "Downloaded: perioda.csv\n",
            "All files downloaded successfully!\n",
            "\n",
            "Loading survey data from GPKG...\n",
            "Survey data loaded: 9401 records\n",
            "Columns: ['lokasi', 'survei', 'sumber', 'bujur', 'lintang', 'kode', 'tanggal', 'fase', 'geometry']\n",
            "Loading period lookup table...\n",
            "Period lookup loaded: 31 records\n",
            "Columns: ['Periode;Start Date;End Date']\n",
            "Sample data:\n",
            "  Periode;Start Date;End Date\n",
            "0     1;2024-01-01;2024-01-13\n",
            "1     2;2024-01-13;2024-01-25\n",
            "2     3;2024-01-25;2024-02-06\n",
            "3     4;2024-02-06;2024-02-18\n",
            "4     5;2024-02-18;2024-03-01\n",
            "\n",
            "Filtering survey data...\n",
            "Available columns: ['lokasi', 'survei', 'sumber', 'bujur', 'lintang', 'kode', 'tanggal', 'fase', 'geometry', 'lon', 'lat']\n",
            "\n",
            "TABEL 1 - Filtered survey data: 9401 records\n",
            "Columns: ['fid', 'lon', 'lat', 'date', 'fase']\n",
            "   fid         lon       lat       date  fase\n",
            "0    1  107.463959 -6.260439  3/18/2024     1\n",
            "1    2  107.464081 -6.260284  3/18/2024     1\n",
            "2    3  107.463608 -6.260762  3/18/2024     1\n",
            "3    4  107.461742 -6.261511  3/18/2024     1\n",
            "4    5  107.463209 -6.260423  3/18/2024     1\n",
            "\n",
            "Adding period information...\n",
            "Period lookup columns: ['Periode;Start Date;End Date']\n",
            "\n",
            "TABEL 2 - Survey data with period: 9401 records\n",
            "Columns: ['fid', 'lon', 'lat', 'date', 'fase', 'periode']\n",
            "   fid         lon       lat       date  fase  periode\n",
            "0    1  107.463959 -6.260439  3/18/2024     1        1\n",
            "1    2  107.464081 -6.260284  3/18/2024     1        1\n",
            "2    3  107.463608 -6.260762  3/18/2024     1        1\n",
            "3    4  107.461742 -6.261511  3/18/2024     1        1\n",
            "4    5  107.463209 -6.260423  3/18/2024     1        1\n",
            "\n",
            "Extracting raster values...\n",
            "Error processing raster: 'downloaded_geotiff.tif' not recognized as being in a supported file format.\n",
            "Returning data without raster values...\n",
            "\n",
            "=== Processing Complete ===\n",
            "Tiga tabel telah berhasil dibuat:\n",
            "1. Tabel 1: Data survey terfilter (fid, lon, lat, date, fase)\n",
            "2. Tabel 2: Data survey dengan periode (fid, lon, lat, date, fase, periode)\n",
            "3. Tabel 3: Data final dengan nilai raster (fid, lon, lat, date, fase, periode, p0)\n",
            "\n",
            "File hasil disimpan sebagai: processed_survey_data.csv\n"
          ]
        }
      ]
    }
  ]
}